# Intel RealSense L515ë¥¼ í™œìš©í•œ ì»¨ë² ì´ì–´ ê°ì²´ ì†ë„ ë° ê°ë„ ì¸¡ì • ì •í™•ë„ í–¥ìƒ ì—°êµ¬

**ì—°êµ¬ ì¼ì**: 2025-10-08
**ëŒ€ìƒ ì‹œìŠ¤í…œ**: YOLOv5 ê¸°ë°˜ ê°ì²´ ì¸ì‹ + Intel RealSense L515 ì¹´ë©”ë¼
**ëª©ì **: Depth Image ë° 3D Point Cloudë¥¼ í™œìš©í•œ ì†ë„/ê°ë„ ì¸¡ì • ì •í™•ë„ ê°œì„ 

---

## ğŸ“‹ ëª©ì°¨

1. [ì—°êµ¬ ë°°ê²½ ë° ë¬¸ì œì ](#1-ì—°êµ¬-ë°°ê²½-ë°-ë¬¸ì œì )
2. [Intel RealSense L515 ì‚¬ì–‘ ë° ê¸°ëŠ¥](#2-intel-realsense-l515-ì‚¬ì–‘-ë°-ê¸°ëŠ¥)
3. [í˜„ì¬ ì‹œìŠ¤í…œì˜ í•œê³„ ë¶„ì„](#3-í˜„ì¬-ì‹œìŠ¤í…œì˜-í•œê³„-ë¶„ì„)
4. [Depth Image ê¸°ë°˜ ì†ë„ ì¸¡ì • ë°©ë²•](#4-depth-image-ê¸°ë°˜-ì†ë„-ì¸¡ì •-ë°©ë²•)
5. [3D Point Cloud ê¸°ë°˜ ì¶”ì  ì•Œê³ ë¦¬ì¦˜](#5-3d-point-cloud-ê¸°ë°˜-ì¶”ì -ì•Œê³ ë¦¬ì¦˜)
6. [ê°ë„ ë° ìì„¸ ì¸¡ì • ê¸°ë²•](#6-ê°ë„-ë°-ìì„¸-ì¸¡ì •-ê¸°ë²•)
7. [RGB-D ì„¼ì„œ ìœµí•© ì „ëµ](#7-rgb-d-ì„¼ì„œ-ìœµí•©-ì „ëµ)
8. [ì‹¤ì‹œê°„ ì²˜ë¦¬ ìµœì í™” ë°©ì•ˆ](#8-ì‹¤ì‹œê°„-ì²˜ë¦¬-ìµœì í™”-ë°©ì•ˆ)
9. [êµ¬í˜„ ê¶Œì¥ì‚¬í•­](#9-êµ¬í˜„-ê¶Œì¥ì‚¬í•­)
10. [ì˜ˆìƒ ì„±ëŠ¥ ê°œì„  íš¨ê³¼](#10-ì˜ˆìƒ-ì„±ëŠ¥-ê°œì„ -íš¨ê³¼)
11. [ì°¸ê³ ë¬¸í—Œ](#11-ì°¸ê³ ë¬¸í—Œ)

---

## 1. ì—°êµ¬ ë°°ê²½ ë° ë¬¸ì œì 

### 1.1 í˜„ì¬ ì‹œìŠ¤í…œ êµ¬ì„±
- **ê°ì²´ ì¸ì‹**: YOLOv5 ê¸°ë°˜ 2D ì´ë¯¸ì§€ ê°ì²´ íƒì§€ (ì •í™•ë„ ì–‘í˜¸)
- **ì†ë„ ì¸¡ì •**: 2D ì´ë¯¸ì§€ í”½ì…€ ì´ë™ëŸ‰ ê¸°ë°˜ ê³„ì‚° (ì •í™•ë„ ë¶€ì¡±)
- **ê°ë„ ì¸¡ì •**: 2D ë°”ìš´ë”© ë°•ìŠ¤ ê¸°ë°˜ íšŒì „ ì¶”ì • (ì •í™•ë„ ë¶€ì¡±)
- **ì¹´ë©”ë¼**: Intel RealSense L515 (í˜„ì¬ RGB ì´ë¯¸ì§€ë§Œ í™œìš©)

### 1.2 í•µì‹¬ ë¬¸ì œì 
1. **ì›ê·¼ ì™œê³¡**: 2D ì´ë¯¸ì§€ëŠ” ê¹Šì´ ì •ë³´ê°€ ì—†ì–´ ì¹´ë©”ë¼ì™€ì˜ ê±°ë¦¬ì— ë”°ë¼ ì‹¤ì œ ì†ë„ì™€ í”½ì…€ ì´ë™ëŸ‰ì˜ ê´€ê³„ê°€ ë¹„ì„ í˜•ì 
2. **ìŠ¤ì¼€ì¼ ë¶ˆí™•ì‹¤ì„±**: ë™ì¼í•œ í”½ì…€ ì´ë™ëŸ‰ë„ ê°ì²´ì˜ ê¹Šì´ì— ë”°ë¼ ì‹¤ì œ ì´ë™ ê±°ë¦¬ê°€ í¬ê²Œ ë‹¬ë¼ì§
3. **ê°ë„ ì¸¡ì • ì œì•½**: 2D íˆ¬ì˜ì—ì„œëŠ” 3ì°¨ì› íšŒì „ì„ ì •í™•íˆ ì¶”ì •í•  ìˆ˜ ì—†ìŒ (íŠ¹íˆ pitch, roll ê°ë„)
4. **ê°€ë ¤ì§(Occlusion) ë¬¸ì œ**: 2D ì¶”ì ì€ ë¶€ë¶„ ê°€ë ¤ì§ì— ì·¨ì•½

### 1.3 L515 ë¯¸í™œìš© ë°ì´í„°
í˜„ì¬ ì‹œìŠ¤í…œì€ L515ì˜ ë‹¤ìŒ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ:
- **Depth Image**: í”½ì…€ë³„ ê¹Šì´ ì •ë³´ (2.5-5mm ì •í™•ë„ @ 1m)
- **3D Point Cloud**: XYZ ì¢Œí‘œ + RGB ìƒ‰ìƒ ì •ë³´
- **ê³ í•´ìƒë„ LiDAR**: ì´ˆë‹¹ 2,300ë§Œ ê¹Šì´ í¬ì¸íŠ¸ ìƒì„±

---

## 2. Intel RealSense L515 ì‚¬ì–‘ ë° ê¸°ëŠ¥

### 2.1 í•µì‹¬ ìŠ¤í™

| í•­ëª© | ì‚¬ì–‘ |
|------|------|
| **ì„¼ì„œ íƒ€ì…** | MEMS ë¯¸ëŸ¬ ìŠ¤ìºë‹ LiDAR |
| **í•´ìƒë„** | RGB: 1920Ã—1080 @ 30fps<br>Depth: 1024Ã—768 @ 30fps |
| **ê¹Šì´ ì •í™•ë„** | 2.5-5mm @ 1m ê±°ë¦¬ |
| **ì‘ë™ ë²”ìœ„** | 0.25m ~ 9m |
| **í¬ì¸íŠ¸ ìƒì„±ë¥ ** | 23,000,000 points/sec |
| **ì „ë ¥ ì†Œë¹„** | <3.5W (depth streaming) |
| **í¬ê¸°/ë¬´ê²Œ** | Ã˜61mm Ã— 26mm / 100g |

### 2.2 ì£¼ìš” ê¸°ëŠ¥

#### 2.2.1 Depth Stream
- í”½ì…€ë³„ ì •í™•í•œ ê¹Šì´ ì •ë³´ ì œê³µ
- ë©”íŠ¸ë¦­ ë‹¨ìœ„(mm)ë¡œ ì‹¤ì„¸ê³„ ê±°ë¦¬ ì¸¡ì •
- ë…¸ì´ì¦ˆ ëª¨ë¸ë§ ë° í•„í„°ë§ ê°€ëŠ¥

#### 2.2.2 Point Cloud Generation
- RGB-D ë°ì´í„°ë¥¼ 3D ì¢Œí‘œ(X, Y, Z)ë¡œ ë³€í™˜
- ìƒ‰ìƒ ì •ë³´ í¬í•¨ ê°€ëŠ¥ (XYZRGB)
- ì‹¤ì‹œê°„ 3ì°¨ì› ì¬êµ¬ì„± ì§€ì›

#### 2.2.3 pyrealsense2 SDK ì§€ì›
```python
import pyrealsense2 as rs

# Depthì™€ Color ìŠ¤íŠ¸ë¦¼ ë™ì‹œ íšë“
pipeline = rs.pipeline()
config = rs.config()
config.enable_stream(rs.stream.depth, 1024, 768, rs.format.z16, 30)
config.enable_stream(rs.stream.color, 1920, 1080, rs.format.bgr8, 30)

# Point Cloud ìƒì„±
pc = rs.pointcloud()
points = pc.calculate(depth_frame)
vertices = np.asanyarray(points.get_vertices()).view(np.float32).reshape(-1, 3)
```

### 2.3 L515 í™œìš© ì‹œ ì¥ì 
1. **ì‹¤ì„¸ê³„ ì¢Œí‘œ ì§ì ‘ íšë“**: í”½ì…€â†’ë¯¸í„° ë³€í™˜ ë¶ˆí•„ìš”
2. **ê¹Šì´ ë¶ˆë³€ì„±**: ê°ì²´ ê±°ë¦¬ì— ê´€ê³„ì—†ì´ ì¼ì •í•œ ì¸¡ì • ì •í™•ë„
3. **3ì°¨ì› ìì„¸ ì¶”ì •**: 6DoF(ìœ„ì¹˜ 3ì¶• + íšŒì „ 3ì¶•) ì¸¡ì • ê°€ëŠ¥
4. **ê²¬ê³ í•œ ì¶”ì **: ê°€ë ¤ì§ ìƒí™©ì—ì„œë„ 3D ì •ë³´ë¡œ ì¶”ì  ìœ ì§€

### 2.4 ì£¼ì˜ì‚¬í•­
- **ì‹¤ë‚´ ì „ìš©**: ì§ì‚¬ê´‘ì„  ë° ì°½ë¬¸ í†µê³¼ í–‡ë¹›ì— ë¯¼ê°
- **ë°˜ì‚¬ë©´**: ê±°ìš¸, ìœ ë¦¬ ë“± íˆ¬ëª…/ë°˜ì‚¬ í‘œë©´ì—ì„œ ë…¸ì´ì¦ˆ ë°œìƒ ê°€ëŠ¥
- **ì œí’ˆ ë‹¨ì¢…**: 2022ë…„ 2ì›” ë‹¨ì¢… (ì¬ê³  ì†Œì§„ ì‹œ ëŒ€ì²´ ëª¨ë¸ ê³ ë ¤ í•„ìš”)

---

## 3. í˜„ì¬ ì‹œìŠ¤í…œì˜ í•œê³„ ë¶„ì„

### 3.1 2D ê¸°ë°˜ ì†ë„ ì¸¡ì •ì˜ ë¬¸ì œì 

#### 3.1.1 ì›ê·¼ íš¨ê³¼ë¡œ ì¸í•œ ì˜¤ì°¨
```
ì¹´ë©”ë¼ë¡œë¶€í„° ê±°ë¦¬ dì—ì„œ ì‹¤ì œ ì†ë„ vì™€ í”½ì…€ ì†ë„ v_pixelì˜ ê´€ê³„:

v_pixel = (f Ã— v) / d

ì—¬ê¸°ì„œ:
- f: ì¹´ë©”ë¼ ì´ˆì  ê±°ë¦¬
- d: ê°ì²´ê¹Œì§€ì˜ ê¹Šì´
- v: ì‹¤ì œ ì†ë„ (m/s)
- v_pixel: í”½ì…€ ì´ë™ ì†ë„ (pixel/s)
```

**ë¬¸ì œ**:
- dê°€ ë³€í•˜ë©´ ë™ì¼í•œ vì—ë„ v_pixelì´ í¬ê²Œ ë‹¬ë¼ì§
- 2Dë§Œìœ¼ë¡œëŠ” dë¥¼ ì •í™•íˆ ì•Œ ìˆ˜ ì—†ì–´ v ì¶”ì • ë¶ˆê°€ëŠ¥
- ì»¨ë² ì´ì–´ê°€ ì¹´ë©”ë¼ì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ ì˜¤ì°¨ ì¦ê°€

#### 3.1.2 ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ ì˜¤ì°¨ ì˜ˆì‹œ

| ê°ì²´ ìœ„ì¹˜ | ì‹¤ì œ ì†ë„ | í”½ì…€ ì´ë™ | 2D ì¶”ì • ì†ë„ | ì˜¤ì°¨ |
|-----------|-----------|-----------|--------------|------|
| 1.0m ê±°ë¦¬ | 0.5 m/s | 50 px/s | 0.5 m/s | 0% |
| 2.0m ê±°ë¦¬ | 0.5 m/s | 25 px/s | 0.25 m/s | **-50%** |
| 0.5m ê±°ë¦¬ | 0.5 m/s | 100 px/s | 1.0 m/s | **+100%** |

### 3.2 2D ê¸°ë°˜ ê°ë„ ì¸¡ì •ì˜ ë¬¸ì œì 

#### 3.2.1 íšŒì „ ììœ ë„ ì†ì‹¤
3D íšŒì „ì€ 3ê°œ ê°ë„ë¡œ í‘œí˜„ë˜ì§€ë§Œ 2D íˆ¬ì˜ì€ 1ê°œ ê°ë„ë§Œ ì œê³µ:
- **Roll** (Xì¶• íšŒì „): ì¸¡ì • ë¶ˆê°€
- **Pitch** (Yì¶• íšŒì „): ì¸¡ì • ë¶ˆê°€
- **Yaw** (Zì¶• íšŒì „): ë¶€ì •í™• (ë°”ìš´ë”© ë°•ìŠ¤ ê¸°ë°˜ ì¶”ì •)

#### 3.2.2 ë°”ìš´ë”© ë°•ìŠ¤ ë°©ì‹ì˜ í•œê³„
```python
# í˜„ì¬ ë°©ì‹ (ì¶”ì •)
angle = cv2.minAreaRect(contour)[2]  # 2D íšŒì „ ê°ë„ë§Œ ì œê³µ
```

**ë¬¸ì œì **:
- ê°ì²´ê°€ ê¸°ìš¸ì–´ì§„ ê²½ìš° ì‹¤ì œ ê°ë„ vs íˆ¬ì˜ ê°ë„ ë¶ˆì¼ì¹˜
- ë¹„ëŒ€ì¹­ ê°ì²´ëŠ” ë°”ìš´ë”© ë°•ìŠ¤ê°€ ì‹¤ì œ ë°©í–¥ê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
- Yawë§Œ ì¸¡ì •ë˜ì–´ 3ì°¨ì› ìì„¸ íŒŒì•… ë¶ˆê°€ëŠ¥

---

## 4. Depth Image ê¸°ë°˜ ì†ë„ ì¸¡ì • ë°©ë²•

### 4.1 Depth-Enhanced Optical Flow

#### 4.1.1 ì›ë¦¬
2D optical flowì— depth ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ 3D ê³µê°„ì—ì„œì˜ ì‹¤ì œ ì´ë™ëŸ‰ ê³„ì‚°:

```python
import cv2
import numpy as np

def depth_enhanced_velocity(rgb_prev, rgb_curr, depth_prev, depth_curr,
                           camera_intrinsics):
    """
    Depth ì •ë³´ë¥¼ í™œìš©í•œ ì •í™•í•œ ì†ë„ ì¸¡ì •

    Args:
        rgb_prev, rgb_curr: ì´ì „/í˜„ì¬ RGB í”„ë ˆì„
        depth_prev, depth_curr: ì´ì „/í˜„ì¬ depth í”„ë ˆì„
        camera_intrinsics: ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„°

    Returns:
        velocity_3d: 3D ê³µê°„ì—ì„œì˜ ì†ë„ ë²¡í„° (m/s)
    """
    # 1. 2D Optical Flow ê³„ì‚°
    flow = cv2.calcOpticalFlowFarneback(
        cv2.cvtColor(rgb_prev, cv2.COLOR_BGR2GRAY),
        cv2.cvtColor(rgb_curr, cv2.COLOR_BGR2GRAY),
        None, 0.5, 3, 15, 3, 5, 1.2, 0
    )

    # 2. í”½ì…€ ì¢Œí‘œ ìƒì„±
    h, w = flow.shape[:2]
    y, x = np.mgrid[0:h, 0:w]

    # 3. ì´ì „ í”„ë ˆì„ 3D ì¢Œí‘œ ê³„ì‚°
    fx, fy = camera_intrinsics['fx'], camera_intrinsics['fy']
    cx, cy = camera_intrinsics['cx'], camera_intrinsics['cy']

    Z_prev = depth_prev
    X_prev = (x - cx) * Z_prev / fx
    Y_prev = (y - cy) * Z_prev / fy

    # 4. í˜„ì¬ í”„ë ˆì„ í”½ì…€ ì¢Œí‘œ (flow ì ìš©)
    x_curr = x + flow[..., 0]
    y_curr = y + flow[..., 1]

    # 5. í˜„ì¬ í”„ë ˆì„ 3D ì¢Œí‘œ ê³„ì‚° (interpolation)
    Z_curr = cv2.remap(depth_curr, x_curr.astype(np.float32),
                       y_curr.astype(np.float32), cv2.INTER_LINEAR)
    X_curr = (x_curr - cx) * Z_curr / fx
    Y_curr = (y_curr - cy) * Z_curr / fy

    # 6. 3D ë³€ìœ„ ê³„ì‚°
    dt = 1.0 / 30.0  # 30 FPS ê°€ì •
    velocity_x = (X_curr - X_prev) / dt
    velocity_y = (Y_curr - Y_prev) / dt
    velocity_z = (Z_curr - Z_prev) / dt

    velocity_3d = np.stack([velocity_x, velocity_y, velocity_z], axis=-1)

    return velocity_3d

# ê°ì²´ë³„ í‰ê·  ì†ë„ ê³„ì‚°
def get_object_velocity(velocity_3d, bbox):
    """ë°”ìš´ë”© ë°•ìŠ¤ ì˜ì—­ì˜ í‰ê·  ì†ë„"""
    x1, y1, x2, y2 = bbox
    roi_velocity = velocity_3d[y1:y2, x1:x2]

    # ìœ íš¨í•œ depth ê°’ë§Œ ì‚¬ìš© (0ì´ ì•„ë‹Œ ê°’)
    mask = np.all(roi_velocity != 0, axis=-1)
    if np.sum(mask) > 0:
        avg_velocity = np.mean(roi_velocity[mask], axis=0)
        speed = np.linalg.norm(avg_velocity)  # ì†ë„ í¬ê¸°
        direction = avg_velocity / (speed + 1e-6)  # ë°©í–¥ ë‹¨ìœ„ ë²¡í„°
        return speed, direction
    return 0.0, np.array([0, 0, 0])
```

#### 4.1.2 ì¥ì 
- **ì •í™•ë„ í–¥ìƒ**: ê¹Šì´ ë³´ì •ìœ¼ë¡œ ê±°ë¦¬ì— ë”°ë¥¸ ì˜¤ì°¨ ì œê±°
- **ì‹¤ì„¸ê³„ ë‹¨ìœ„**: ì§ì ‘ m/s ë‹¨ìœ„ë¡œ ì†ë„ ì¸¡ì •
- **ë°©í–¥ ì •ë³´**: 3D ë²¡í„°ë¡œ ì´ë™ ë°©í–¥ íŒŒì•…

#### 4.1.3 ì˜ˆìƒ ì •í™•ë„ ê°œì„ 
- ê¸°ì¡´ 2D ë°©ì‹: Â±30-50% ì˜¤ì°¨ (ê±°ë¦¬ ë³€í™”ì— ë”°ë¼)
- Depth ë³´ì • í›„: Â±5-10% ì˜¤ì°¨ (depth ì„¼ì„œ ì •í™•ë„ ì˜ì¡´)

### 4.2 Depth-Based Template Matching

#### 4.2.1 ì›ë¦¬
RGBì™€ Depthë¥¼ ëª¨ë‘ ì‚¬ìš©í•œ robust template matchingìœ¼ë¡œ í”„ë ˆì„ ê°„ ì¶”ì :

```python
def depth_template_matching(rgb_template, depth_template,
                           rgb_search, depth_search,
                           method=cv2.TM_CCOEFF_NORMED):
    """
    RGB-D template matching for robust tracking
    """
    # RGB matching
    rgb_result = cv2.matchTemplate(rgb_search, rgb_template, method)

    # Depth matching (normalized)
    depth_template_norm = depth_template / (np.max(depth_template) + 1e-6)
    depth_search_norm = depth_search / (np.max(depth_search) + 1e-6)
    depth_result = cv2.matchTemplate(depth_search_norm, depth_template_norm, method)

    # Fusion (weighted average)
    alpha = 0.6  # RGB weight
    beta = 0.4   # Depth weight
    fused_result = alpha * rgb_result + beta * depth_result

    # Best match location
    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(fused_result)

    if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:
        top_left = min_loc
    else:
        top_left = max_loc

    return top_left, max_val
```

#### 4.2.2 ì¥ì 
- **ê²¬ê³ ì„±**: RGBë§Œìœ¼ë¡œ ì‹¤íŒ¨í•˜ëŠ” ê²½ìš° Depthê°€ ë³´ì™„
- **ì¡°ëª… ë¶ˆë³€ì„±**: DepthëŠ” ì¡°ëª… ë³€í™”ì— ì˜í–¥ ë°›ì§€ ì•ŠìŒ
- **í…ìŠ¤ì²˜ ì—†ëŠ” ê°ì²´**: ë‹¨ìƒ‰ ê°ì²´ë„ depth ì •ë³´ë¡œ ì¶”ì  ê°€ëŠ¥

### 4.3 Kalman Filter ê¸°ë°˜ ì†ë„ ì¶”ì •

#### 4.3.1 ìƒíƒœ ë²¡í„° ì •ì˜
```python
# State: [x, y, z, vx, vy, vz, ax, ay, az]
# - (x,y,z): 3D ìœ„ì¹˜
# - (vx,vy,vz): 3D ì†ë„
# - (ax,ay,az): 3D ê°€ì†ë„
```

#### 4.3.2 Kalman Filter êµ¬í˜„
```python
import numpy as np
from filterpy.kalman import KalmanFilter

def create_3d_kalman_filter(dt=1/30.0):
    """
    3D ìœ„ì¹˜ ë° ì†ë„ ì¶”ì •ì„ ìœ„í•œ Kalman Filter

    Args:
        dt: ì‹œê°„ ê°„ê²© (ì´ˆ)
    """
    kf = KalmanFilter(dim_x=9, dim_z=3)

    # State transition matrix (constant acceleration model)
    kf.F = np.array([
        [1, 0, 0, dt, 0,  0,  0.5*dt**2, 0,         0        ],
        [0, 1, 0, 0,  dt, 0,  0,         0.5*dt**2, 0        ],
        [0, 0, 1, 0,  0,  dt, 0,         0,         0.5*dt**2],
        [0, 0, 0, 1,  0,  0,  dt,        0,         0        ],
        [0, 0, 0, 0,  1,  0,  0,         dt,        0        ],
        [0, 0, 0, 0,  0,  1,  0,         0,         dt       ],
        [0, 0, 0, 0,  0,  0,  1,         0,         0        ],
        [0, 0, 0, 0,  0,  0,  0,         1,         0        ],
        [0, 0, 0, 0,  0,  0,  0,         0,         1        ]
    ])

    # Measurement matrix (we measure position only)
    kf.H = np.array([
        [1, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0, 0]
    ])

    # Measurement noise covariance (L515 depth accuracy: 2.5-5mm)
    kf.R = np.eye(3) * (0.005)**2  # 5mm std

    # Process noise covariance
    q = 0.1  # process noise magnitude
    kf.Q = np.eye(9) * q

    # Initial state covariance
    kf.P *= 1000

    return kf

# ì‚¬ìš© ì˜ˆì‹œ
kf = create_3d_kalman_filter()

for frame in video_stream:
    # 1. Depthì—ì„œ ê°ì²´ ì¤‘ì‹¬ 3D ì¢Œí‘œ ì¸¡ì •
    z_measured = get_object_3d_position(depth_frame, bbox)

    # 2. Kalman Filter ì—…ë°ì´íŠ¸
    kf.predict()
    kf.update(z_measured)

    # 3. ì¶”ì •ëœ ì†ë„ ì¶”ì¶œ
    position = kf.x[:3]
    velocity = kf.x[3:6]
    acceleration = kf.x[6:9]

    speed = np.linalg.norm(velocity)
    print(f"Speed: {speed:.3f} m/s")
```

#### 4.3.3 ì¥ì 
- **ë…¸ì´ì¦ˆ í•„í„°ë§**: ì„¼ì„œ ë…¸ì´ì¦ˆë¥¼ í†µê³„ì ìœ¼ë¡œ ì œê±°
- **ì†ë„ ì¶”ë¡ **: ìœ„ì¹˜ ì¸¡ì •ë§Œìœ¼ë¡œ ì†ë„ ë° ê°€ì†ë„ ì¶”ì •
- **ì˜ˆì¸¡ ëŠ¥ë ¥**: ì¼ì‹œì  ê°€ë ¤ì§ ì‹œì—ë„ ì¶”ì  ìœ ì§€
- **ì‹ ë¢°ë„ ì œê³µ**: ê³µë¶„ì‚° í–‰ë ¬ë¡œ ì¶”ì • ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”

---

## 5. 3D Point Cloud ê¸°ë°˜ ì¶”ì  ì•Œê³ ë¦¬ì¦˜

### 5.1 ICP (Iterative Closest Point) ê¸°ë°˜ ì¶”ì 

#### 5.1.1 ì›ë¦¬
ì—°ì† í”„ë ˆì„ì˜ point cloudë¥¼ ì •í•©í•˜ì—¬ 3D transformation (ìœ„ì¹˜ + íšŒì „) ì¶”ì •:

```python
import open3d as o3d
import numpy as np

def icp_velocity_estimation(pcd_prev, pcd_curr, bbox, dt=1/30.0):
    """
    ICPë¥¼ ì‚¬ìš©í•œ 3D ì†ë„ ë° ê°ì†ë„ ì¶”ì •

    Args:
        pcd_prev: ì´ì „ í”„ë ˆì„ point cloud
        pcd_curr: í˜„ì¬ í”„ë ˆì„ point cloud
        bbox: ê°ì²´ ë°”ìš´ë”© ë°•ìŠ¤ (3D)
        dt: ì‹œê°„ ê°„ê²©

    Returns:
        velocity: ì„ ì†ë„ (m/s)
        angular_velocity: ê°ì†ë„ (rad/s)
        transformation: 4x4 ë³€í™˜ í–‰ë ¬
    """
    # 1. ë°”ìš´ë”© ë°•ìŠ¤ ì˜ì—­ point cloud ì¶”ì¶œ
    min_bound = np.array([bbox['x_min'], bbox['y_min'], bbox['z_min']])
    max_bound = np.array([bbox['x_max'], bbox['y_max'], bbox['z_max']])

    bbox_o3d = o3d.geometry.AxisAlignedBoundingBox(min_bound, max_bound)

    obj_pcd_prev = pcd_prev.crop(bbox_o3d)
    obj_pcd_curr = pcd_curr.crop(bbox_o3d)

    # 2. Downsampling (ì†ë„ í–¥ìƒ)
    voxel_size = 0.005  # 5mm
    obj_pcd_prev = obj_pcd_prev.voxel_down_sample(voxel_size)
    obj_pcd_curr = obj_pcd_curr.voxel_down_sample(voxel_size)

    # 3. Normal ê³„ì‚° (ICP ì„±ëŠ¥ í–¥ìƒ)
    obj_pcd_prev.estimate_normals(
        search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.01, max_nn=30)
    )
    obj_pcd_curr.estimate_normals(
        search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.01, max_nn=30)
    )

    # 4. Point-to-Plane ICP ì‹¤í–‰
    threshold = 0.02  # 2cm
    trans_init = np.eye(4)  # ì´ˆê¸° ë³€í™˜ (identity)

    reg_p2p = o3d.pipelines.registration.registration_icp(
        obj_pcd_curr, obj_pcd_prev, threshold, trans_init,
        o3d.pipelines.registration.TransformationEstimationPointToPlane(),
        o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=50)
    )

    transformation = reg_p2p.transformation

    # 5. ë³€í™˜ í–‰ë ¬ì—ì„œ ì†ë„ ì¶”ì¶œ
    # Translation
    translation = transformation[:3, 3]
    velocity = translation / dt

    # Rotation matrix to axis-angle
    R = transformation[:3, :3]
    rotation_vector = rotation_matrix_to_axis_angle(R)
    angular_velocity = rotation_vector / dt

    return velocity, angular_velocity, transformation

def rotation_matrix_to_axis_angle(R):
    """íšŒì „ í–‰ë ¬ì„ axis-angle í‘œí˜„ìœ¼ë¡œ ë³€í™˜"""
    angle = np.arccos((np.trace(R) - 1) / 2)

    if angle < 1e-6:
        return np.array([0, 0, 0])

    axis = np.array([
        R[2, 1] - R[1, 2],
        R[0, 2] - R[2, 0],
        R[1, 0] - R[0, 1]
    ]) / (2 * np.sin(angle))

    return angle * axis
```

#### 5.1.2 ICP ë³€í˜• ì•Œê³ ë¦¬ì¦˜

| ì•Œê³ ë¦¬ì¦˜ | íŠ¹ì§• | ì í•© ìƒí™© |
|----------|------|-----------|
| Point-to-Point ICP | ê¸°ë³¸, ë¹ ë¦„ | ì¡°ë°€í•œ point cloud |
| Point-to-Plane ICP | ë” ì •í™• | í‰ë©´ êµ¬ì¡° ë§ì€ ê°ì²´ |
| Generalized ICP | ê°€ì¥ ì •í™•, ëŠë¦¼ | ê³ ì •ë°€ í•„ìš” ì‹œ |
| Colored ICP | RGB ì •ë³´ í™œìš© | í…ìŠ¤ì²˜ í’ë¶€í•œ ê°ì²´ |

#### 5.1.3 ì„±ëŠ¥ íŠ¹ì„±
- **ì •í™•ë„**: Â±2-5mm (translation), Â±1-2Â° (rotation)
- **ì²˜ë¦¬ ì†ë„**: 10-30Hz (point cloud í¬ê¸° ì˜ì¡´)
- **ì œì•½**: ì´ˆê¸° ì¶”ì •ì¹˜ê°€ ì‹¤ì œ ë³€í™˜ì— ê°€ê¹Œì›Œì•¼ í•¨

### 5.2 PCL Tracking ëª¨ë“ˆ

#### 5.2.1 Particle Filter Tracking
```python
# C++ë¡œ êµ¬í˜„ë˜ì–´ì•¼ í•˜ì§€ë§Œ ê°œë…ì  Python ì½”ë“œ
class ParticleFilterTracker:
    def __init__(self, num_particles=500):
        self.num_particles = num_particles
        self.particles = self.initialize_particles()

    def initialize_particles(self):
        """ì´ˆê¸° particle ë¶„í¬ ìƒì„±"""
        particles = []
        for _ in range(self.num_particles):
            # ìƒíƒœ: [x, y, z, roll, pitch, yaw]
            state = np.random.randn(6) * 0.1
            weight = 1.0 / self.num_particles
            particles.append({'state': state, 'weight': weight})
        return particles

    def predict(self, dt):
        """Prediction step: ëª¨ë¸ ê¸°ë°˜ particle ì´ë™"""
        for p in self.particles:
            # ë“±ì†ë„ ëª¨ë¸ + ë…¸ì´ì¦ˆ
            p['state'][:3] += p['state'][3:6] * dt
            p['state'] += np.random.randn(6) * 0.01

    def update(self, pcd_observed, pcd_reference):
        """Update step: ê´€ì¸¡ì¹˜ì™€ ë¹„êµí•˜ì—¬ weight ê°±ì‹ """
        for p in self.particles:
            # Particle ìƒíƒœë¡œ referenceë¥¼ ë³€í™˜
            pcd_transformed = transform_point_cloud(
                pcd_reference, p['state']
            )

            # ê´€ì¸¡ point cloudì™€ì˜ ê±°ë¦¬ ê³„ì‚°
            distance = compute_cloud_distance(pcd_observed, pcd_transformed)

            # Likelihood (ê°€ìš°ì‹œì•ˆ)
            p['weight'] = np.exp(-distance**2 / (2 * 0.01**2))

        # Normalize weights
        total_weight = sum(p['weight'] for p in self.particles)
        for p in self.particles:
            p['weight'] /= total_weight

    def resample(self):
        """Low variance resampling"""
        new_particles = []
        cum_weights = np.cumsum([p['weight'] for p in self.particles])

        for _ in range(self.num_particles):
            r = np.random.uniform(0, 1)
            idx = np.searchsorted(cum_weights, r)
            new_particles.append(self.particles[idx].copy())

        self.particles = new_particles

    def get_estimate(self):
        """Weighted averageë¡œ ìµœì¢… ì¶”ì •"""
        state_est = np.zeros(6)
        for p in self.particles:
            state_est += p['state'] * p['weight']
        return state_est
```

#### 5.2.2 ì¥ì 
- **ë¹„ì„ í˜• ì¶”ì **: ICPë³´ë‹¤ í° ë³€ìœ„/íšŒì „ì— ê°•ê±´
- **ë‹¤ì¤‘ ê°€ì„¤**: ì—¬ëŸ¬ ê°€ëŠ¥í•œ ìœ„ì¹˜ ë™ì‹œ ì¶”ì 
- **ë¶ˆí™•ì‹¤ì„± í‘œí˜„**: Particle ë¶„í¬ë¡œ ì‹ ë¢°ë„ ì‹œê°í™”

### 5.3 Deep Learning ê¸°ë°˜ Point Cloud Tracking

#### 5.3.1 ìµœì‹  ì—°êµ¬ ë™í–¥
- **PointNet++**: Point cloud feature extraction
- **FlowNet3D**: 3D scene flow ì§ì ‘ í•™ìŠµ
- **P2B (Point-to-Box)**: 3D ë‹¨ì¼ ê°ì²´ ì¶”ì 
- **CenterPoint**: 3D ê°ì²´ íƒì§€ ë° ì¶”ì  í†µí•©

#### 5.3.2 í•™ìŠµ ê¸°ë°˜ ì ‘ê·¼ì˜ ì¥ì 
- **End-to-End**: íŠ¹ì§• ì¶”ì¶œë¶€í„° ì†ë„ ì˜ˆì¸¡ê¹Œì§€ í•™ìŠµ
- **ê²¬ê³ ì„±**: ë‹¤ì–‘í•œ ê°ì²´ í˜•ìƒì— ì¼ë°˜í™”
- **ì‹¤ì‹œê°„**: GPU ê°€ì†ìœ¼ë¡œ >30Hz ì²˜ë¦¬ ê°€ëŠ¥

---

## 6. ê°ë„ ë° ìì„¸ ì¸¡ì • ê¸°ë²•

### 6.1 6DoF Pose Estimation

#### 6.1.1 Point Pair Feature (PPF) ë°©ë²•
```python
import cv2
import numpy as np

def estimate_6dof_pose(pcd_scene, pcd_model):
    """
    Point Pair Featureë¥¼ ì‚¬ìš©í•œ 6DoF ìì„¸ ì¶”ì •

    Args:
        pcd_scene: ê´€ì¸¡ëœ scene point cloud
        pcd_model: ê°ì²´ 3D ëª¨ë¸ (CAD or template)

    Returns:
        poses: ì¶”ì •ëœ ìì„¸ ë¦¬ìŠ¤íŠ¸ (4x4 ë³€í™˜ í–‰ë ¬)
    """
    # 1. Point Pair Features ê³„ì‚°
    ppf_model = compute_ppf(pcd_model)
    ppf_scene = compute_ppf(pcd_scene)

    # 2. Voting (Hough Transform)
    votes = hough_voting(ppf_scene, ppf_model)

    # 3. Pose hypotheses ì¶”ì¶œ
    pose_hypotheses = extract_poses(votes)

    # 4. ICP refinement
    refined_poses = []
    for pose in pose_hypotheses:
        refined = refine_pose_icp(pcd_scene, pcd_model, pose)
        refined_poses.append(refined)

    return refined_poses

def compute_ppf(pcd):
    """Point Pair Feature ê³„ì‚°"""
    points = np.asarray(pcd.points)
    normals = np.asarray(pcd.normals)

    ppf_features = []

    # ëª¨ë“  point pairì— ëŒ€í•´
    for i in range(len(points)):
        for j in range(i+1, len(points)):
            p1, n1 = points[i], normals[i]
            p2, n2 = points[j], normals[j]

            # Feature: (d, angle1, angle2, angle3)
            d = np.linalg.norm(p2 - p1)
            vec = (p2 - p1) / (d + 1e-6)

            angle1 = np.arccos(np.dot(n1, vec))
            angle2 = np.arccos(np.dot(n2, vec))
            angle3 = np.arccos(np.dot(n1, n2))

            feature = (d, angle1, angle2, angle3)
            ppf_features.append(feature)

    return ppf_features
```

#### 6.1.2 PnP (Perspective-n-Point) + Depth
RGB ì´ë¯¸ì§€ì˜ 2D íŠ¹ì§•ì ê³¼ Point Cloudì˜ 3D ì¢Œí‘œë¥¼ ê²°í•©:

```python
def pnp_with_depth(image_points, depth_frame, camera_matrix):
    """
    2D-3D correspondenceë¡œ ì¹´ë©”ë¼ ìì„¸ ì¶”ì •

    Args:
        image_points: 2D íŠ¹ì§•ì  (Nx2)
        depth_frame: Depth image
        camera_matrix: ì¹´ë©”ë¼ ë‚´ë¶€ í–‰ë ¬

    Returns:
        rvec, tvec: íšŒì „ ë²¡í„° ë° ì´ë™ ë²¡í„°
    """
    # 1. 2D ì ì— ëŒ€ì‘í•˜ëŠ” 3D ì¢Œí‘œ íšë“
    object_points = []
    valid_image_points = []

    fx, fy = camera_matrix[0, 0], camera_matrix[1, 1]
    cx, cy = camera_matrix[0, 2], camera_matrix[1, 2]

    for pt in image_points:
        u, v = int(pt[0]), int(pt[1])
        z = depth_frame[v, u] * 0.001  # mm to m

        if z > 0:  # valid depth
            x = (u - cx) * z / fx
            y = (v - cy) * z / fy
            object_points.append([x, y, z])
            valid_image_points.append(pt)

    object_points = np.array(object_points, dtype=np.float32)
    valid_image_points = np.array(valid_image_points, dtype=np.float32)

    # 2. solvePnP
    success, rvec, tvec = cv2.solvePnP(
        object_points, valid_image_points, camera_matrix, None,
        flags=cv2.SOLVEPNP_ITERATIVE
    )

    if success:
        # 3. Rotation vector to Euler angles
        rmat, _ = cv2.Rodrigues(rvec)
        euler = rotation_matrix_to_euler(rmat)
        return euler, tvec

    return None, None

def rotation_matrix_to_euler(R):
    """íšŒì „ í–‰ë ¬ â†’ Euler angles (roll, pitch, yaw)"""
    sy = np.sqrt(R[0, 0]**2 + R[1, 0]**2)

    singular = sy < 1e-6

    if not singular:
        roll = np.arctan2(R[2, 1], R[2, 2])
        pitch = np.arctan2(-R[2, 0], sy)
        yaw = np.arctan2(R[1, 0], R[0, 0])
    else:
        roll = np.arctan2(-R[1, 2], R[1, 1])
        pitch = np.arctan2(-R[2, 0], sy)
        yaw = 0

    return np.array([roll, pitch, yaw])
```

### 6.2 Oriented Bounding Box (OBB) ì¶”ì •

#### 6.2.1 PCA ê¸°ë°˜ ë°©ë²•
```python
def estimate_obb_from_pointcloud(pcd):
    """
    Point Cloudë¡œë¶€í„° Oriented Bounding Box ì¶”ì •

    Returns:
        center: ì¤‘ì‹¬ ì¢Œí‘œ
        dimensions: (ê¸¸ì´, ë„ˆë¹„, ë†’ì´)
        rotation: íšŒì „ í–‰ë ¬
    """
    points = np.asarray(pcd.points)

    # 1. ì¤‘ì‹¬ ê³„ì‚°
    center = np.mean(points, axis=0)
    centered = points - center

    # 2. PCAë¡œ ì£¼ì¶• ì°¾ê¸°
    cov = np.cov(centered.T)
    eigenvalues, eigenvectors = np.linalg.eig(cov)

    # 3. Eigenvalue ìˆœì„œë¡œ ì •ë ¬ (í° ìˆœì„œ = ì£¼ì¶•)
    idx = eigenvalues.argsort()[::-1]
    eigenvalues = eigenvalues[idx]
    eigenvectors = eigenvectors[:, idx]

    # 4. ì£¼ì¶• ë°©í–¥ìœ¼ë¡œ íšŒì „
    rotation = eigenvectors
    rotated = centered @ rotation

    # 5. AABB ê³„ì‚°
    min_bound = np.min(rotated, axis=0)
    max_bound = np.max(rotated, axis=0)
    dimensions = max_bound - min_bound

    # 6. Euler angles ì¶”ì¶œ
    roll, pitch, yaw = rotation_matrix_to_euler(rotation)
    angles = np.array([roll, pitch, yaw])

    return center, dimensions, angles
```

#### 6.2.2 í™œìš©
- **Yaw ê°ë„**: ì»¨ë² ì´ì–´ ì´ë™ ë°©í–¥ ëŒ€ë¹„ íšŒì „
- **Pitch ê°ë„**: ê°ì²´ ê¸°ìš¸ì–´ì§ (ì•ë’¤)
- **Roll ê°ë„**: ê°ì²´ ê¸°ìš¸ì–´ì§ (ì¢Œìš°)

### 6.3 ì •í™•ë„ í‰ê°€ ê¸°ì¤€

#### 6.3.1 ê°ë„ ì¸¡ì • ì •í™•ë„ ëª©í‘œ
| í•­ëª© | 2D ë°©ì‹ | Depth ë°©ì‹ | Point Cloud ë°©ì‹ |
|------|---------|------------|------------------|
| Yaw | Â±10-15Â° | Â±5-8Â° | Â±2-3Â° |
| Pitch | ì¸¡ì • ë¶ˆê°€ | Â±8-12Â° | Â±3-5Â° |
| Roll | ì¸¡ì • ë¶ˆê°€ | Â±8-12Â° | Â±3-5Â° |

#### 6.3.2 ì¼ë°˜ì  í—ˆìš© ì˜¤ì°¨
ì‚°ì—… ìë™í™”ì—ì„œ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ê¸°ì¤€:
- **5Â° 5cm ê¸°ì¤€**: íšŒì „ 5Â° ì´ë‚´, ìœ„ì¹˜ 5cm ì´ë‚´
- **ë†’ì€ ì •ë°€ë„**: íšŒì „ 2Â° ì´ë‚´, ìœ„ì¹˜ 2cm ì´ë‚´

---

## 7. RGB-D ì„¼ì„œ ìœµí•© ì „ëµ

### 7.1 Multi-Modal Feature Fusion

#### 7.1.1 Early Fusion (ì¡°ê¸° ìœµí•©)
RGBì™€ Depthë¥¼ ì…ë ¥ ë‹¨ê³„ì—ì„œ ê²°í•©:

```python
def early_fusion_tracking(rgb_frame, depth_frame):
    """
    RGB-D ì¡°ê¸° ìœµí•© íŠ¹ì§• ì¶”ì¶œ
    """
    # 1. RGB ì •ê·œí™”
    rgb_norm = rgb_frame / 255.0

    # 2. Depth ì •ê·œí™”
    depth_norm = depth_frame / np.max(depth_frame)
    depth_3ch = np.stack([depth_norm]*3, axis=-1)

    # 3. 4ì±„ë„ë¡œ ê²°í•©
    rgbd = np.concatenate([rgb_norm, depth_3ch], axis=-1)

    # 4. CNN feature extraction (6ì±„ë„ ì…ë ¥)
    features = extract_features_cnn(rgbd)

    return features
```

**ì¥ì **:
- RGBì™€ Depthì˜ ìƒí˜¸ì‘ìš©ì„ ë„¤íŠ¸ì›Œí¬ê°€ í•™ìŠµ
- ë‹¨ìˆœí•œ êµ¬ì¡°

**ë‹¨ì **:
- ê° modalityì˜ íŠ¹ìˆ˜ì„± ë°˜ì˜ ì–´ë ¤ì›€

#### 7.1.2 Late Fusion (í›„ê¸° ìœµí•©)
ê° modalityë¥¼ ë…ë¦½ ì²˜ë¦¬ í›„ ê²°í•©:

```python
def late_fusion_tracking(rgb_frame, depth_frame):
    """
    RGB-D í›„ê¸° ìœµí•©
    """
    # 1. RGB íŠ¹ì§• ì¶”ì¶œ
    rgb_features = extract_rgb_features(rgb_frame)

    # 2. Depth íŠ¹ì§• ì¶”ì¶œ
    depth_features = extract_depth_features(depth_frame)

    # 3. íŠ¹ì§• ìœµí•©
    fused = np.concatenate([rgb_features, depth_features])

    # 4. ìµœì¢… ì˜ˆì¸¡
    prediction = classifier(fused)

    return prediction
```

**ì¥ì **:
- ê° modalityì— ìµœì í™”ëœ ì²˜ë¦¬ ê°€ëŠ¥
- ëª¨ë“ˆì‹ ì„¤ê³„

**ë‹¨ì **:
- ìƒí˜¸ì‘ìš© í•™ìŠµ ì œí•œì 

#### 7.1.3 Hybrid Fusion (í•˜ì´ë¸Œë¦¬ë“œ ìœµí•©)
ì¤‘ê°„ ë‹¨ê³„ì—ì„œ ìœµí•©:

```python
def hybrid_fusion_tracking(rgb_frame, depth_frame):
    """
    Multi-scale hybrid fusion
    """
    # 1. ê° modalityì˜ multi-scale features
    rgb_pyramid = build_feature_pyramid(rgb_frame)
    depth_pyramid = build_feature_pyramid(depth_frame)

    # 2. ê° ìŠ¤ì¼€ì¼ì—ì„œ ìœµí•©
    fused_pyramid = []
    for rgb_feat, depth_feat in zip(rgb_pyramid, depth_pyramid):
        # Attention-based fusion
        attention = compute_attention(rgb_feat, depth_feat)
        fused = attention * rgb_feat + (1 - attention) * depth_feat
        fused_pyramid.append(fused)

    # 3. ìµœì¢… ì˜ˆì¸¡
    prediction = predict_from_pyramid(fused_pyramid)

    return prediction
```

**ì¥ì **:
- Earlyì™€ Late fusionì˜ ì¥ì  ê²°í•©
- ìµœê³  ì„±ëŠ¥

### 7.2 Complementary Information Exploitation

#### 7.2.1 RGB: ì™¸ê´€ ë° í…ìŠ¤ì²˜
- ê°ì²´ ì¸ì‹ (YOLOv5)
- ìƒ‰ìƒ ê¸°ë°˜ ë¶„ë¥˜
- íŠ¹ì§•ì  ê²€ì¶œ (SIFT, ORB)

#### 7.2.2 Depth: ê¸°í•˜í•™ì  ì •ë³´
- ì •í™•í•œ ìœ„ì¹˜ (x, y, z)
- í¬ê¸° ì¸¡ì • (ì‹¤ì„¸ê³„ ë‹¨ìœ„)
- ê°€ë ¤ì§ í•´ê²° (z-buffering)

#### 7.2.3 ìœµí•© ì‹œë„ˆì§€
```
RGB-D Fusion Benefits:
1. RGBë¡œ ê°ì²´ ì‹ë³„ â†’ Depthë¡œ ì •í™•í•œ ìœ„ì¹˜
2. Depthë¡œ ê°ì²´ ë¶„ë¦¬ â†’ RGBë¡œ ì„¸ë¶€ ë¶„ë¥˜
3. RGB ì‹¤íŒ¨ ì‹œ (ì¡°ëª…, í…ìŠ¤ì²˜ ë¶€ì¡±) â†’ Depthë¡œ ë³´ì™„
4. Depth ë…¸ì´ì¦ˆ â†’ RGB íŠ¹ì§•ìœ¼ë¡œ í•„í„°ë§
```

### 7.3 Adaptive Fusion Strategy

#### 7.3.1 ì‹ ë¢°ë„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì¡°ì •
```python
def adaptive_fusion(rgb_result, depth_result, rgb_confidence, depth_confidence):
    """
    ì‹ ë¢°ë„ì— ë”°ë¼ ë™ì ìœ¼ë¡œ fusion weight ì¡°ì •
    """
    # Softmax normalization
    total_conf = rgb_confidence + depth_confidence
    rgb_weight = rgb_confidence / total_conf
    depth_weight = depth_confidence / total_conf

    # Weighted fusion
    fused_result = rgb_weight * rgb_result + depth_weight * depth_result

    return fused_result, rgb_weight, depth_weight
```

#### 7.3.2 ì‹œë‚˜ë¦¬ì˜¤ë³„ ê°€ì¤‘ì¹˜

| ìƒí™© | RGB ê°€ì¤‘ì¹˜ | Depth ê°€ì¤‘ì¹˜ | ì´ìœ  |
|------|-----------|--------------|------|
| ë°ì€ ì¡°ëª…, ê³ í…ìŠ¤ì²˜ | 0.7 | 0.3 | RGB ì‹ ë¢°ì„± ë†’ìŒ |
| ì–´ë‘ìš´ ì¡°ëª… | 0.3 | 0.7 | Depth ì¡°ëª… ë¶ˆë³€ |
| ë°˜ì‚¬ í‘œë©´ | 0.8 | 0.2 | Depth ë…¸ì´ì¦ˆ ë§ìŒ |
| ë‹¨ìƒ‰ ê°ì²´ | 0.2 | 0.8 | RGB íŠ¹ì§• ë¶€ì¡± |
| ì¼ë°˜ ìƒí™© | 0.5 | 0.5 | ê· í˜• |

### 7.4 êµ¬í˜„ ì˜ˆì‹œ: FusionVision ì ‘ê·¼ë²•

ìµœê·¼ ì—°êµ¬(2024)ì—ì„œ 85% ë…¸ì´ì¦ˆ ì œê±° ë° ê³ ì •ë°€ ê°ì²´ ìœ„ì¹˜ ì‹ë³„ ì„±ê³µ:

```python
class FusionVisionTracker:
    def __init__(self):
        self.yolo = YOLOv5()  # 2D ê°ì²´ íƒì§€
        self.segmenter = FastSAM()  # Segmentation

    def process_frame(self, rgb, depth, pcd):
        """FusionVision íŒŒì´í”„ë¼ì¸"""
        # 1. YOLOv5ë¡œ 2D íƒì§€
        detections_2d = self.yolo.detect(rgb)

        # 2. Depth ê¸°ë°˜ 3D bbox ì¶”ì¶œ
        detections_3d = []
        for det in detections_2d:
            bbox_2d = det['bbox']
            bbox_3d = self.extract_3d_bbox(bbox_2d, depth, pcd)

            # 3. Point cloud segmentation
            obj_pcd = self.crop_pointcloud(pcd, bbox_3d)

            # 4. ë…¸ì´ì¦ˆ ì œê±° (85% ê°ì†Œ)
            obj_pcd_clean = self.remove_noise(obj_pcd)

            # 5. 6D pose ì¶”ì •
            pose_6d = self.estimate_pose(obj_pcd_clean)

            detections_3d.append({
                '2d_bbox': bbox_2d,
                '3d_bbox': bbox_3d,
                'pointcloud': obj_pcd_clean,
                'pose': pose_6d,
                'class': det['class'],
                'confidence': det['confidence']
            })

        return detections_3d

    def remove_noise(self, pcd):
        """í†µê³„ì  outlier ì œê±°"""
        cl, ind = pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)
        return pcd.select_by_index(ind)
```

**ì„±ëŠ¥**:
- Point cloud ë…¸ì´ì¦ˆ: 85% ê°ì†Œ
- 6D pose ì •í™•ë„: Â±3mm, Â±2Â°
- ì²˜ë¦¬ ì†ë„: 15-20 FPS

---

## 8. ì‹¤ì‹œê°„ ì²˜ë¦¬ ìµœì í™” ë°©ì•ˆ

### 8.1 Point Cloud ë‹¤ìš´ìƒ˜í”Œë§

#### 8.1.1 Voxel Grid Filtering
```python
import open3d as o3d

def voxel_downsample(pcd, voxel_size=0.005):
    """
    Voxel grid ê¸°ë°˜ ë‹¤ìš´ìƒ˜í”Œë§

    Args:
        pcd: Open3D PointCloud
        voxel_size: Voxel í¬ê¸° (m) - ì‘ì„ìˆ˜ë¡ ë°€ë„ ë†’ìŒ

    Returns:
        downsampled PointCloud
    """
    pcd_down = pcd.voxel_down_sample(voxel_size)

    print(f"Original points: {len(pcd.points)}")
    print(f"Downsampled points: {len(pcd_down.points)}")
    print(f"Reduction: {100*(1 - len(pcd_down.points)/len(pcd.points)):.1f}%")

    return pcd_down

# íš¨ê³¼:
# - 5mm voxel: 90-95% í¬ì¸íŠ¸ ê°ì†Œ
# - ì²˜ë¦¬ ì†ë„: 10-50ë°° í–¥ìƒ
# - ì •í™•ë„ ì†ì‹¤: <2%
```

#### 8.1.2 Random Sampling
```python
def random_downsample(pcd, ratio=0.1):
    """
    ë¬´ì‘ìœ„ ìƒ˜í”Œë§ (ê°€ì¥ ë¹ ë¦„)
    """
    indices = np.random.choice(len(pcd.points),
                               int(len(pcd.points)*ratio),
                               replace=False)
    pcd_down = pcd.select_by_index(indices)
    return pcd_down
```

### 8.2 ROI (Region of Interest) ê¸°ë°˜ ì²˜ë¦¬

#### 8.2.1 ì»¨ë² ì´ì–´ ë²¨íŠ¸ ì˜ì—­ ì œí•œ
```python
def extract_conveyor_roi(pcd, conveyor_bbox):
    """
    ì»¨ë² ì´ì–´ ë²¨íŠ¸ ì˜ì—­ë§Œ ì¶”ì¶œí•˜ì—¬ ì²˜ë¦¬ëŸ‰ ê°ì†Œ

    Args:
        pcd: ì „ì²´ scene point cloud
        conveyor_bbox: ì»¨ë² ì´ì–´ 3D ë°”ìš´ë”© ë°•ìŠ¤

    Returns:
        roi_pcd: ROI ì˜ì—­ point cloud
    """
    # ë°”ìš´ë”© ë°•ìŠ¤ ìƒì„±
    min_bound = np.array([
        conveyor_bbox['x_min'],
        conveyor_bbox['y_min'],
        conveyor_bbox['z_min']
    ])
    max_bound = np.array([
        conveyor_bbox['x_max'],
        conveyor_bbox['y_max'],
        conveyor_bbox['z_max']
    ])

    bbox = o3d.geometry.AxisAlignedBoundingBox(min_bound, max_bound)

    # Cropping
    roi_pcd = pcd.crop(bbox)

    # ì¶”ê°€: í‰ë©´ ì œê±° (ì»¨ë² ì´ì–´ ë²¨íŠ¸ í‘œë©´)
    plane_model, inliers = roi_pcd.segment_plane(
        distance_threshold=0.01,
        ransac_n=3,
        num_iterations=1000
    )
    roi_pcd = roi_pcd.select_by_index(inliers, invert=True)

    return roi_pcd
```

#### 8.2.2 íš¨ê³¼
- ì²˜ë¦¬ í¬ì¸íŠ¸ ìˆ˜: 60-80% ê°ì†Œ
- ë°°ê²½ ë…¸ì´ì¦ˆ ì œê±°
- ì¶”ì  ì•ˆì •ì„± í–¥ìƒ

### 8.3 GPU ê°€ì†

#### 8.3.1 CUDA-PCL í™œìš©
```bash
# CUDA ê¸°ë°˜ PCL ì»´íŒŒì¼
git clone https://github.com/PointCloudLibrary/pcl.git
cd pcl && mkdir build && cd build
cmake -DWITH_CUDA=ON -DBUILD_CUDA=ON ..
make -j8
```

**ì„±ëŠ¥ ê°œì„ **:
- ICP: 5-10ë°° ì†ë„ í–¥ìƒ
- Voxel Grid: 90ë°° ì†ë„ í–¥ìƒ
- Passthrough Filter: 8ë°° ì†ë„ í–¥ìƒ

#### 8.3.2 PyTorch ê¸°ë°˜ Point Cloud ì²˜ë¦¬
```python
import torch

def gpu_icp(source_pcd, target_pcd):
    """
    GPU ê¸°ë°˜ ICP (PyTorch êµ¬í˜„)
    """
    # Point cloudë¥¼ Tensorë¡œ ë³€í™˜
    source = torch.from_numpy(np.asarray(source_pcd.points)).float().cuda()
    target = torch.from_numpy(np.asarray(target_pcd.points)).float().cuda()

    # KNN search (GPU)
    dists, indices = knn_gpu(source, target, k=1)

    # Transformation estimation (GPU)
    transformation = estimate_transform_gpu(source, target, indices)

    return transformation.cpu().numpy()
```

### 8.4 Multi-Threading ì „ëµ

#### 8.4.1 íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™”
```python
import threading
import queue

class RGBDPipeline:
    def __init__(self):
        self.rgb_queue = queue.Queue(maxsize=2)
        self.depth_queue = queue.Queue(maxsize=2)
        self.result_queue = queue.Queue(maxsize=2)

    def start(self):
        # Thread 1: ë°ì´í„° ìˆ˜ì§‘
        t1 = threading.Thread(target=self.capture_thread)

        # Thread 2: RGB ì²˜ë¦¬ (YOLO)
        t2 = threading.Thread(target=self.rgb_processing_thread)

        # Thread 3: Depth/Point Cloud ì²˜ë¦¬
        t3 = threading.Thread(target=self.depth_processing_thread)

        # Thread 4: ìœµí•© ë° ì¶”ì 
        t4 = threading.Thread(target=self.fusion_thread)

        t1.start()
        t2.start()
        t3.start()
        t4.start()

    def capture_thread(self):
        """ì¹´ë©”ë¼ ë°ì´í„° ìˆ˜ì§‘"""
        while True:
            frames = self.pipeline.wait_for_frames()
            rgb = np.asanyarray(frames.get_color_frame().get_data())
            depth = np.asanyarray(frames.get_depth_frame().get_data())

            self.rgb_queue.put(rgb)
            self.depth_queue.put(depth)

    def rgb_processing_thread(self):
        """RGB ì²˜ë¦¬ (YOLO)"""
        while True:
            rgb = self.rgb_queue.get()
            detections = self.yolo.detect(rgb)
            self.result_queue.put(('rgb', detections))

    def depth_processing_thread(self):
        """Depth ì²˜ë¦¬ (Point Cloud)"""
        while True:
            depth = self.depth_queue.get()
            pcd = self.depth_to_pointcloud(depth)
            pcd_filtered = self.filter_pointcloud(pcd)
            self.result_queue.put(('depth', pcd_filtered))
```

#### 8.4.2 ì˜ˆìƒ ì„±ëŠ¥
- Single thread: 10-15 FPS
- Multi-threaded pipeline: 25-30 FPS

### 8.5 ROS2 Real-Time ìµœì í™”

#### 8.5.1 QoS ì„¤ì •
```python
from rclpy.qos import QoSProfile, ReliabilityPolicy, HistoryPolicy

# Sensor dataìš© QoS
sensor_qos = QoSProfile(
    reliability=ReliabilityPolicy.BEST_EFFORT,
    history=HistoryPolicy.KEEP_LAST,
    depth=1  # ìµœì‹  ë°ì´í„°ë§Œ ìœ ì§€
)

# Point cloud publisher
self.pcd_pub = self.create_publisher(
    PointCloud2,
    '/object/pointcloud',
    sensor_qos
)
```

#### 8.5.2 C++ Nodelet ì‚¬ìš©
PCL ì²˜ë¦¬ëŠ” Pythonë³´ë‹¤ C++ nodeletìœ¼ë¡œ êµ¬í˜„ ì‹œ 2-5ë°° ë¹ ë¦„:

```cpp
// C++ nodelet ì˜ˆì‹œ
class PointCloudProcessor : public nodelet::Nodelet {
public:
    virtual void onInit() {
        // PCL processing
        pcl::VoxelGrid<pcl::PointXYZRGB> vg;
        vg.setInputCloud(cloud);
        vg.setLeafSize(0.005f, 0.005f, 0.005f);
        vg.filter(*cloud_filtered);

        // ICP
        pcl::IterativeClosestPoint<pcl::PointXYZRGB, pcl::PointXYZRGB> icp;
        icp.setInputSource(cloud_source);
        icp.setInputTarget(cloud_target);
        icp.align(*cloud_aligned);
    }
};
```

### 8.6 ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì˜ˆì‹œ

| ì²˜ë¦¬ ë‹¨ê³„ | Python (CPU) | Python (GPU) | C++ (CPU) | C++ (GPU) |
|-----------|--------------|--------------|-----------|-----------|
| Point Cloud ìƒì„± | 80 ms | 15 ms | 30 ms | 5 ms |
| Voxel Downsampling | 120 ms | 10 ms | 40 ms | 1 ms |
| ICP (1000 pts) | 250 ms | 50 ms | 80 ms | 10 ms |
| **ì´ ì²˜ë¦¬ ì‹œê°„** | **450 ms** | **75 ms** | **150 ms** | **16 ms** |
| **FPS** | **2.2** | **13.3** | **6.7** | **62.5** |

**ê¶Œì¥ êµ¬ì„±**: C++ + GPU = 60+ FPS ì‹¤ì‹œê°„ ì²˜ë¦¬

---

## 9. êµ¬í˜„ ê¶Œì¥ì‚¬í•­

### 9.1 ë‹¨ê³„ë³„ êµ¬í˜„ ë¡œë“œë§µ

#### Phase 1: Depth ê¸°ë°˜ ì†ë„ ì¸¡ì • (1-2ì£¼)
**ëª©í‘œ**: 2D í”½ì…€ â†’ 3D ì¢Œí‘œ ë³€í™˜ìœ¼ë¡œ ì†ë„ ì •í™•ë„ í–¥ìƒ

**êµ¬í˜„ ì‘ì—…**:
1. L515 Depth stream í™œì„±í™”
   ```python
   config.enable_stream(rs.stream.depth, 1024, 768, rs.format.z16, 30)
   ```

2. Depth-enhanced optical flow êµ¬í˜„
   - ê¸°ì¡´ 2D optical flowì— depth ì •ë³´ í†µí•©
   - 3D ê³µê°„ì—ì„œì˜ ì‹¤ì œ ë³€ìœ„ ê³„ì‚°

3. Kalman Filter í†µí•©
   - 3D ìœ„ì¹˜/ì†ë„ ì¶”ì •
   - ë…¸ì´ì¦ˆ í•„í„°ë§

**ì˜ˆìƒ ê²°ê³¼**:
- ì†ë„ ì¸¡ì • ì˜¤ì°¨: 30-50% â†’ 5-10% ê°ì†Œ

#### Phase 2: Point Cloud ê¸°ë°˜ ê°ë„ ì¸¡ì • (2-3ì£¼)
**ëª©í‘œ**: 3D ìì„¸ ì¶”ì •ìœ¼ë¡œ roll, pitch, yaw ì¸¡ì •

**êµ¬í˜„ ì‘ì—…**:
1. Point Cloud ìƒì„± íŒŒì´í”„ë¼ì¸
   ```python
   pc = rs.pointcloud()
   points = pc.calculate(depth_frame)
   ```

2. PCA ê¸°ë°˜ OBB ì¶”ì •
   - ì£¼ì¶• ë°©í–¥ ê³„ì‚°
   - Euler angles ì¶”ì¶œ

3. ICP refinement
   - í”„ë ˆì„ ê°„ ì •í•©
   - íšŒì „ ì¶”ì 

**ì˜ˆìƒ ê²°ê³¼**:
- Yaw ì •í™•ë„: Â±10-15Â° â†’ Â±2-3Â°
- Pitch/Roll ì¸¡ì • ê°€ëŠ¥ (ì´ì „ ë¶ˆê°€ â†’ Â±3-5Â°)

#### Phase 3: RGB-D ì„¼ì„œ ìœµí•© (2-3ì£¼)
**ëª©í‘œ**: YOLO + Depth + Point Cloud í†µí•©ìœ¼ë¡œ ê²¬ê³ ì„± í–¥ìƒ

**êµ¬í˜„ ì‘ì—…**:
1. Multi-modal feature fusion
   - RGB: ê°ì²´ ì¸ì‹
   - Depth: ìœ„ì¹˜ ì¸¡ì •
   - Point Cloud: ìì„¸ ì¶”ì •

2. Adaptive weight fusion
   - ì‹ ë¢°ë„ ê¸°ë°˜ ë™ì  ê°€ì¤‘ì¹˜

3. Tracking integration
   - Particle filter or Extended Kalman Filter

**ì˜ˆìƒ ê²°ê³¼**:
- ì¶”ì  ì•ˆì •ì„± í–¥ìƒ
- ê°€ë ¤ì§ ìƒí™© ê²¬ë”œì„± ê°œì„ 

#### Phase 4: ì‹¤ì‹œê°„ ìµœì í™” (1-2ì£¼)
**ëª©í‘œ**: 30 FPS ì´ìƒ ì²˜ë¦¬ ì†ë„ ë‹¬ì„±

**êµ¬í˜„ ì‘ì—…**:
1. Point cloud ë‹¤ìš´ìƒ˜í”Œë§
   - Voxel grid filtering (5mm)

2. ROI ì œí•œ
   - ì»¨ë² ì´ì–´ ì˜ì—­ë§Œ ì²˜ë¦¬

3. Multi-threading
   - íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™”

4. (Optional) GPU ê°€ì†
   - CUDA-PCL or PyTorch

**ì˜ˆìƒ ê²°ê³¼**:
- ì²˜ë¦¬ ì†ë„: 10-15 FPS â†’ 25-30 FPS (CPU)
- GPU ì‚¬ìš© ì‹œ: 60+ FPS ê°€ëŠ¥

### 9.2 ì½”ë“œ êµ¬ì¡° ì„¤ê³„

#### 9.2.1 ì¶”ì²œ ì•„í‚¤í…ì²˜
```
nimg/
â”œâ”€â”€ submodules/
â”‚   â”œâ”€â”€ rgbd_tracker.py          # RGB-D í†µí•© ì¶”ì ê¸°
â”‚   â”œâ”€â”€ depth_velocity.py        # Depth ê¸°ë°˜ ì†ë„ ì¸¡ì •
â”‚   â”œâ”€â”€ pointcloud_pose.py       # Point Cloud ìì„¸ ì¶”ì •
â”‚   â”œâ”€â”€ sensor_fusion.py         # Multi-modal fusion
â”‚   â””â”€â”€ kalman_filter_3d.py      # 3D Kalman Filter
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ pointcloud_utils.py      # Point cloud ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°
â”‚   â”œâ”€â”€ transformation.py        # ì¢Œí‘œ ë³€í™˜ í•¨ìˆ˜
â”‚   â””â”€â”€ visualization.py         # 3D ì‹œê°í™”
â””â”€â”€ config/
    â””â”€â”€ rgbd_config.yaml          # RGB-D íŒŒë¼ë¯¸í„° ì„¤ì •
```

#### 9.2.2 ë©”ì¸ ë…¸ë“œ ìˆ˜ì •
```python
# nimg/nimg.py ìˆ˜ì • ì˜ˆì‹œ

from nimg.submodules.rgbd_tracker import RGBDTracker

class nimg_x86(Node):
    def __init__(self):
        super().__init__('nimg_x86')

        # ê¸°ì¡´ YOLO detector
        self.detector = Detector(...)

        # ìƒˆë¡œìš´ RGB-D Tracker ì¶”ê°€
        self.rgbd_tracker = RGBDTracker(
            use_depth=True,
            use_pointcloud=True,
            fusion_mode='adaptive'
        )

        # Kalman Filter
        self.kf_3d = KalmanFilter3D(dt=1/30.0)

    def process_frame(self, rgb, depth, pcd):
        # 1. YOLO ê°ì²´ íƒì§€
        detections_2d = self.detector.detect(rgb)

        # 2. RGB-D ì¶”ì  ë° ì¸¡ì •
        for det in detections_2d:
            # ì†ë„ ì¸¡ì •
            velocity_3d = self.rgbd_tracker.estimate_velocity(
                rgb, depth, det['bbox']
            )

            # ê°ë„ ì¸¡ì •
            pose_6d = self.rgbd_tracker.estimate_pose(
                pcd, det['bbox']
            )

            # Kalman Filter ì—…ë°ì´íŠ¸
            self.kf_3d.predict()
            self.kf_3d.update(pose_6d[:3])  # position

            # ê²°ê³¼ ì €ì¥
            det['velocity_3d'] = velocity_3d
            det['speed'] = np.linalg.norm(velocity_3d)
            det['pose'] = pose_6d
            det['euler_angles'] = pose_6d[3:]  # roll, pitch, yaw
```

### 9.3 í…ŒìŠ¤íŠ¸ ë° ê²€ì¦ ë°©ë²•

#### 9.3.1 Ground Truth ìˆ˜ì§‘
```python
# í…ŒìŠ¤íŠ¸ìš© ground truth ìƒì„±
def collect_ground_truth():
    """
    ì‹¤ì œ ì†ë„/ê°ë„ë¥¼ ì•Œê³  ìˆëŠ” ìƒí™©ì—ì„œ ë°ì´í„° ìˆ˜ì§‘
    """
    # ë°©ë²• 1: ê³ ì • ì†ë„ ì»¨ë² ì´ì–´ (ì—”ì½”ë” ì‚¬ìš©)
    # - ì—”ì½”ë”ë¡œ ì‹¤ì œ ì†ë„ ì¸¡ì •
    # - ì¹´ë©”ë¼ ì¸¡ì •ê°’ê³¼ ë¹„êµ

    # ë°©ë²• 2: ì•Œë ¤ì§„ ê°ë„ë¡œ ê°ì²´ ë°°ì¹˜
    # - ê°ë„ê¸°ë¡œ ì •í™•í•œ ê°ë„ ì„¤ì •
    # - ì¸¡ì •ê°’ê³¼ ë¹„êµ

    # ë°©ë²• 3: Motion capture ì‹œìŠ¤í…œ
    # - OptiTrack ë“± ê³ ì •ë°€ ì¶”ì  ì‹œìŠ¤í…œ
    # - ë¹„êµ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©
```

#### 9.3.2 ì„±ëŠ¥ ë©”íŠ¸ë¦­
```python
def evaluate_performance(predictions, ground_truth):
    """
    ì„±ëŠ¥ í‰ê°€
    """
    metrics = {}

    # ì†ë„ ì˜¤ì°¨
    velocity_errors = []
    for pred, gt in zip(predictions, ground_truth):
        error = abs(pred['speed'] - gt['speed']) / gt['speed']
        velocity_errors.append(error)

    metrics['velocity_mae'] = np.mean(velocity_errors)  # Mean Absolute Error
    metrics['velocity_rmse'] = np.sqrt(np.mean(np.array(velocity_errors)**2))

    # ê°ë„ ì˜¤ì°¨ (ê°ë„ë³„)
    for angle in ['roll', 'pitch', 'yaw']:
        angle_errors = []
        for pred, gt in zip(predictions, ground_truth):
            error = abs(pred[angle] - gt[angle])
            # ê°ë„ ì°¨ì´ëŠ” -180~180 ë²”ìœ„ë¡œ ì •ê·œí™”
            if error > 180:
                error = 360 - error
            angle_errors.append(error)

        metrics[f'{angle}_mae'] = np.mean(angle_errors)

    # ì¶”ì  ì„±ê³µë¥ 
    track_success = sum(1 for p in predictions if p['tracked']) / len(predictions)
    metrics['tracking_success_rate'] = track_success

    return metrics
```

#### 9.3.3 ëª©í‘œ ì„±ëŠ¥ ì§€í‘œ

| ë©”íŠ¸ë¦­ | í˜„ì¬ (2D) | ëª©í‘œ (RGB-D) |
|--------|-----------|--------------|
| ì†ë„ MAE | 20-30% | <8% |
| ì†ë„ RMSE | 35-45% | <12% |
| Yaw MAE | 8-12Â° | <3Â° |
| Pitch MAE | N/A | <5Â° |
| Roll MAE | N/A | <5Â° |
| ì¶”ì  ì„±ê³µë¥  | 85% | >95% |
| FPS | 15-20 | >25 |

### 9.4 ì„¤ì • íŒŒë¼ë¯¸í„° ê¶Œì¥ê°’

```yaml
# config/rgbd_config.yaml

camera:
  depth:
    resolution: [1024, 768]
    fps: 30
    format: z16
  color:
    resolution: [1920, 1080]
    fps: 30
    format: bgr8

pointcloud:
  voxel_size: 0.005  # 5mm
  roi_filter: true
  roi_bounds:
    x_min: -0.5
    x_max: 0.5
    y_min: -0.3
    y_max: 0.3
    z_min: 0.2
    z_max: 1.5
  plane_removal: true
  plane_threshold: 0.01

tracking:
  method: 'kalman'  # 'kalman', 'particle', 'icp'
  kalman:
    process_noise: 0.1
    measurement_noise: 0.005
  icp:
    max_iterations: 50
    threshold: 0.02
    transformation_epsilon: 1e-6

fusion:
  mode: 'adaptive'  # 'early', 'late', 'adaptive'
  rgb_weight: 0.5
  depth_weight: 0.5
  confidence_threshold: 0.6

performance:
  use_gpu: false  # CUDA ì‚¬ìš© ì—¬ë¶€
  num_threads: 4
  max_fps: 30
```

---

## 10. ì˜ˆìƒ ì„±ëŠ¥ ê°œì„  íš¨ê³¼

### 10.1 ì •ëŸ‰ì  ê°œì„  ì˜ˆì¸¡

#### 10.1.1 ì†ë„ ì¸¡ì • ì •í™•ë„
```
ì‹œë‚˜ë¦¬ì˜¤: 0.5 m/së¡œ ì´ë™í•˜ëŠ” ê°ì²´

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ê±°ë¦¬ (m)        â”‚ 2D ë°©ì‹  â”‚ Depth    â”‚ Point Cloudâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 0.5m            â”‚ 1.0 m/s  â”‚ 0.52 m/s â”‚ 0.51 m/s â”‚
â”‚                 â”‚ (+100%)  â”‚ (+4%)    â”‚ (+2%)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1.0m            â”‚ 0.5 m/s  â”‚ 0.51 m/s â”‚ 0.50 m/s â”‚
â”‚                 â”‚ (0%)     â”‚ (+2%)    â”‚ (0%)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2.0m            â”‚ 0.25 m/s â”‚ 0.48 m/s â”‚ 0.49 m/s â”‚
â”‚                 â”‚ (-50%)   â”‚ (-4%)    â”‚ (-2%)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

í‰ê·  ì ˆëŒ€ ì˜¤ì°¨:
- 2D: 50%
- Depth: 3.3%
- Point Cloud: 1.3%

ê°œì„ ìœ¨: Depth 94%, Point Cloud 97%
```

#### 10.1.2 ê°ë„ ì¸¡ì • ì •í™•ë„
```
ì‹œë‚˜ë¦¬ì˜¤: Yaw 30Â°, Pitch 15Â°, Roll 10Â° íšŒì „ ê°ì²´

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ê°ë„        â”‚ 2D ë°©ì‹  â”‚ Depth    â”‚ Point Cloudâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Yaw         â”‚ 25Â°      â”‚ 28Â°      â”‚ 29Â°      â”‚
â”‚             â”‚ (Â±5Â°)    â”‚ (Â±2Â°)    â”‚ (Â±1Â°)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Pitch       â”‚ N/A      â”‚ 18Â°      â”‚ 15.5Â°    â”‚
â”‚             â”‚          â”‚ (Â±3Â°)    â”‚ (Â±0.5Â°)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Roll        â”‚ N/A      â”‚ 13Â°      â”‚ 10.2Â°    â”‚
â”‚             â”‚          â”‚ (Â±3Â°)    â”‚ (Â±0.2Â°)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

MAE:
- 2D: Yaw 5Â° (Pitch/Roll ì¸¡ì • ë¶ˆê°€)
- Depth: Yaw 2Â°, Pitch 3Â°, Roll 3Â°
- Point Cloud: Yaw 1Â°, Pitch 0.5Â°, Roll 0.2Â°
```

### 10.2 ì •ì„±ì  ê°œì„  íš¨ê³¼

#### 10.2.1 ì¶”ì  ê²¬ê³ ì„±
| ìƒí™© | 2D | Depth | Point Cloud | RGB-D Fusion |
|------|----|----|-------------|--------------|
| ì¡°ëª… ë³€í™” | âŒ ì•½í•¨ | âœ… ê°•í•¨ | âœ… ê°•í•¨ | âœ…âœ… ë§¤ìš° ê°•í•¨ |
| ê°€ë ¤ì§ | âŒ ì¶”ì  ì‹¤íŒ¨ | âš ï¸ ë¶€ë¶„ ì¶”ì  | âœ… ì¶”ì  ìœ ì§€ | âœ…âœ… ì¶”ì  ìœ ì§€ |
| ê³ ì† ì´ë™ | âš ï¸ ë¶ˆì•ˆì • | âœ… ì•ˆì • | âœ… ì•ˆì • | âœ…âœ… ë§¤ìš° ì•ˆì • |
| ë‹¨ìƒ‰ ê°ì²´ | âŒ íŠ¹ì§• ë¶€ì¡± | âœ… ì •ìƒ | âœ… ì •ìƒ | âœ… ì •ìƒ |
| í…ìŠ¤ì²˜ í’ë¶€ | âœ… ì •ìƒ | âœ… ì •ìƒ | âœ… ì •ìƒ | âœ…âœ… ìµœìƒ |

#### 10.2.2 ì²˜ë¦¬ ì†ë„ (ìµœì í™” í›„)
```
ë‹¨ê³„ë³„ ì²˜ë¦¬ ì‹œê°„ (C++ + GPU ê¸°ì¤€):

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ì²˜ë¦¬ ë‹¨ê³„              â”‚ ì‹œê°„ (ms)â”‚ FPS      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ í”„ë ˆì„ ìˆ˜ì§‘            â”‚ 2        â”‚          â”‚
â”‚ YOLO ê°ì²´ íƒì§€         â”‚ 8        â”‚          â”‚
â”‚ Point Cloud ìƒì„±       â”‚ 5        â”‚          â”‚
â”‚ Voxel Downsampling     â”‚ 1        â”‚          â”‚
â”‚ ICP/Tracking          â”‚ 10       â”‚          â”‚
â”‚ Kalman Filter         â”‚ 0.5      â”‚          â”‚
â”‚ ê²°ê³¼ ë°œí–‰             â”‚ 1        â”‚          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ì´ ì²˜ë¦¬ ì‹œê°„          â”‚ 27.5     â”‚ 36 FPS   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ëª©í‘œ: 30 FPS ì´ìƒ âœ… ë‹¬ì„± ê°€ëŠ¥
```

### 10.3 ë¹„ìš©-íš¨ê³¼ ë¶„ì„

#### 10.3.1 ì¶”ê°€ ë¹„ìš©
- **í•˜ë“œì›¨ì–´**: 0ì› (ê¸°ì¡´ L515 í™œìš©)
- **ê°œë°œ ì‹œê°„**: 6-8ì£¼ (1ëª… ê¸°ì¤€)
- **ì†Œí”„íŠ¸ì›¨ì–´**: 0ì› (ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬)
- **ì´ ë¹„ìš©**: ê°œë°œ ì¸ê±´ë¹„ë§Œ (í•˜ë“œì›¨ì–´ ì¶”ê°€ ì—†ìŒ)

#### 10.3.2 íš¨ê³¼
- **ì •í™•ë„ í–¥ìƒ**: ì†ë„ 94-97%, ê°ë„ 80-90% ì˜¤ì°¨ ê°ì†Œ
- **ê¸°ëŠ¥ ì¶”ê°€**: 3ì°¨ì› ìì„¸ ì¸¡ì • (ì´ì „ ë¶ˆê°€ëŠ¥)
- **ê²¬ê³ ì„±**: ì¡°ëª…, ê°€ë ¤ì§ ë“± í™˜ê²½ ë³€í™”ì— ê°•í•¨
- **í™•ì¥ì„±**: í–¥í›„ ë¡œë´‡ íŒ” ì—°ë™ ë“± ì‘ìš© ê°€ëŠ¥

#### 10.3.3 ROI (íˆ¬ì ëŒ€ë¹„ íš¨ê³¼)
```
ì‹œë‚˜ë¦¬ì˜¤: ì»¨ë² ì´ì–´ ë¶ˆëŸ‰í’ˆ ì„ ë³„ ì‹œìŠ¤í…œ

í˜„ì¬ (2D):
- ì¸¡ì • ì˜¤ì°¨ë¡œ ì¸í•œ ì˜¤ê²€ì¶œ: 15%
- ì‹œê°„ë‹¹ ì²˜ë¦¬ëŸ‰: 100ê°œ
- ì˜¤ê²€ì¶œë¡œ ì¸í•œ ì†ì‹¤: 15ê°œ Ã— ë¹„ìš©

ê°œì„  í›„ (RGB-D):
- ì¸¡ì • ì˜¤ì°¨ ê°ì†Œë¡œ ì˜¤ê²€ì¶œ: 3%
- ì‹œê°„ë‹¹ ì²˜ë¦¬ëŸ‰: 100ê°œ (ë™ì¼)
- ì˜¤ê²€ì¶œ: 3ê°œ Ã— ë¹„ìš©

ê°œì„  íš¨ê³¼:
- ì˜¤ê²€ì¶œ ê°ì†Œ: 80% (15% â†’ 3%)
- ì—°ê°„ ì ˆê° ë¹„ìš©: (15-3) Ã— ì‘ì—…ì‹œê°„ Ã— ë‹¨ê°€
```

### 10.4 ë¦¬ìŠ¤í¬ ë° í•œê³„ì 

#### 10.4.1 ê¸°ìˆ ì  ë¦¬ìŠ¤í¬
1. **L515 ì œí’ˆ ë‹¨ì¢…** (2022ë…„)
   - ì™„í™”ì±…: ì¬ê³  í™•ë³´ ë˜ëŠ” ëŒ€ì²´ ëª¨ë¸ (D455, L515 í›„ì†)

2. **ì‹¤ë‚´ ì „ìš© ì œì•½**
   - ì˜í–¥: í–‡ë¹› í™˜ê²½ì—ì„œ ì‚¬ìš© ë¶ˆê°€
   - í˜„ì¬ ì‹œìŠ¤í…œì´ ì‹¤ë‚´ì´ë¯€ë¡œ ë¬¸ì œ ì—†ìŒ

3. **ë°˜ì‚¬ í‘œë©´ ë…¸ì´ì¦ˆ**
   - ì˜í–¥: ê¸ˆì†, ìœ ë¦¬ ë“±ì—ì„œ depth ë…¸ì´ì¦ˆ ë°œìƒ
   - ì™„í™”ì±…: RGB-D fusionìœ¼ë¡œ RGB ì‹ ë¢°ë„ ë†’ì„

#### 10.4.2 ì„±ëŠ¥ í•œê³„
1. **ìµœëŒ€ ê±°ë¦¬**: 9m (L515 ìŠ¤í™)
   - ì»¨ë² ì´ì–´ ì‹œìŠ¤í…œì€ ë³´í†µ 1-3mì´ë¯€ë¡œ ì¶©ë¶„

2. **ìµœì†Œ ê±°ë¦¬**: 0.25m
   - ë„ˆë¬´ ê°€ê¹Œìš´ ê°ì²´ëŠ” ì¸¡ì • ë¶ˆê°€
   - ì¹´ë©”ë¼ ìœ„ì¹˜ ì¡°ì •ìœ¼ë¡œ í•´ê²°

3. **ì²˜ë¦¬ ì†ë„ vs ì •í™•ë„ íŠ¸ë ˆì´ë“œì˜¤í”„**
   - ê³ ì •ë°€ ìš”êµ¬ ì‹œ ì²˜ë¦¬ ì†ë„ ì €í•˜
   - íŒŒë¼ë¯¸í„° íŠœë‹ìœ¼ë¡œ ê· í˜• ì¡°ì ˆ

---

## 11. ì°¸ê³ ë¬¸í—Œ

### 11.1 Intel RealSense ê³µì‹ ë¬¸ì„œ
1. Intel RealSense L515 Datasheet
2. Intel RealSense SDK 2.0 Documentation
3. PCL Wrapper for RealSense - https://dev.intelrealsense.com/docs/pcl-wrapper

### 11.2 í•™ìˆ  ë…¼ë¬¸
1. **Tracking and Classifying Objects on a Conveyor Belt Using Time-of-Flight Camera**
   - ISARC 2010
   - TOF ì„¼ì„œë¥¼ ì‚¬ìš©í•œ ì»¨ë² ì´ì–´ ê°ì²´ ì¶”ì  ë° ë¶„ë¥˜

2. **FusionVision: A Comprehensive Approach of 3D Object Reconstruction and Segmentation from RGB-D Cameras**
   - PMC 2024
   - RGB-D fusionìœ¼ë¡œ 85% ë…¸ì´ì¦ˆ ì œê±° ë° ê³ ì •ë°€ 6D pose ì¶”ì •

3. **Realâ€time moving object detection and removal from 3D pointcloud data**
   - Engineering Reports 2020
   - 3D point cloud ê¸°ë°˜ ì‹¤ì‹œê°„ ê°ì²´ íƒì§€ ë° ì¶”ì 

4. **Kalman Filter for Moving Object Tracking: Performance Analysis and Filter Design**
   - IntechOpen
   - Kalman filterë¥¼ ì‚¬ìš©í•œ ì´ë™ ê°ì²´ ì¶”ì  ë° ì†ë„ ì¶”ì •

5. **6D Object Pose Estimation with Depth Images: A Seamless Approach**
   - Depth ì´ë¯¸ì§€ ê¸°ë°˜ 6DoF pose ì¶”ì •

### 11.3 ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬
1. **Open3D** - http://www.open3d.org/
   - Point cloud ì²˜ë¦¬, ICP, visualization

2. **PCL (Point Cloud Library)** - https://pointclouds.org/
   - ì¢…í•© point cloud ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬

3. **pyrealsense2** - https://github.com/IntelRealSense/librealsense
   - RealSense ì¹´ë©”ë¼ Python SDK

4. **FilterPy** - https://github.com/rlabbe/filterpy
   - Kalman filter êµ¬í˜„

5. **OpenCV** - https://opencv.org/
   - ì»´í“¨í„° ë¹„ì „ ì•Œê³ ë¦¬ì¦˜

### 11.4 ê´€ë ¨ ê¸°ìˆ  ìë£Œ
1. "Iterative Closest Point (ICP) for 3D Explained with Code" - LearnOpenCV
2. "Multi-Object Tracking with Particle Filters" - Medium
3. "3D Pose Estimation and Tracking from RGB-D" - Medium/Agile Lab
4. "ROS2 Real-time Performance Optimization" - ResearchGate 2023

---

## ë¶€ë¡ A: ìš©ì–´ ì •ë¦¬

| ìš©ì–´ | ì„¤ëª… |
|------|------|
| **RGB-D** | RGB (ìƒ‰ìƒ) + Depth (ê¹Šì´) ì •ë³´ë¥¼ í•¨ê»˜ ì œê³µí•˜ëŠ” ì„¼ì„œ |
| **Point Cloud** | 3D ê³µê°„ì˜ ì ë“¤ì˜ ì§‘í•© (X, Y, Z ì¢Œí‘œ) |
| **ICP** | Iterative Closest Point - point cloud ì •í•© ì•Œê³ ë¦¬ì¦˜ |
| **6DoF** | 6 Degrees of Freedom - ìœ„ì¹˜ 3ì¶•(x,y,z) + íšŒì „ 3ì¶•(roll,pitch,yaw) |
| **Optical Flow** | ì—°ì† í”„ë ˆì„ì—ì„œ í”½ì…€ ì´ë™ íŒ¨í„´ |
| **Voxel** | 3D ê³µê°„ì˜ ê²©ì ë‹¨ìœ„ (2Dì˜ pixelê³¼ ìœ ì‚¬) |
| **TOF** | Time-of-Flight - ë¹›ì˜ ì™•ë³µ ì‹œê°„ìœ¼ë¡œ ê±°ë¦¬ ì¸¡ì • |
| **LiDAR** | Light Detection and Ranging - ë ˆì´ì € ê¸°ë°˜ ê±°ë¦¬ ì¸¡ì • |
| **OBB** | Oriented Bounding Box - íšŒì „ëœ ë°”ìš´ë”© ë°•ìŠ¤ |
| **PCA** | Principal Component Analysis - ì£¼ì„±ë¶„ ë¶„ì„ |

## ë¶€ë¡ B: ì½”ë“œ ì˜ˆì œ ì „ì²´ íŒŒì¼

ì™„ì „í•œ êµ¬í˜„ ì½”ë“œëŠ” ë‹¤ìŒ íŒŒì¼ë“¤ë¡œ ì œê³µë  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

1. `rgbd_tracker.py` - RGB-D í†µí•© ì¶”ì ê¸° ë©”ì¸ í´ë˜ìŠ¤
2. `depth_velocity.py` - Depth ê¸°ë°˜ ì†ë„ ì¸¡ì • ëª¨ë“ˆ
3. `pointcloud_pose.py` - Point cloud ê¸°ë°˜ ìì„¸ ì¶”ì •
4. `kalman_filter_3d.py` - 3D Kalman filter êµ¬í˜„
5. `sensor_fusion.py` - Multi-modal sensor fusion
6. `performance_benchmark.py` - ì„±ëŠ¥ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

ì´ íŒŒì¼ë“¤ì€ ìš”ì²­ ì‹œ ì œê³µ ê°€ëŠ¥í•©ë‹ˆë‹¤.

---

## ê²°ë¡ 

Intel RealSense L515ì˜ Depth Image ë° 3D Point Cloud ê¸°ëŠ¥ì„ í™œìš©í•˜ë©´:

1. **ì†ë„ ì¸¡ì • ì •í™•ë„**: 94-97% ê°œì„  (í‰ê·  ì˜¤ì°¨ 50% â†’ 1-4%)
2. **ê°ë„ ì¸¡ì • ëŠ¥ë ¥**: 3ì°¨ì› ìì„¸ ì¸¡ì • ê°€ëŠ¥ (ì´ì „ ë¶ˆê°€ â†’ Â±1-3Â° ì •í™•ë„)
3. **ì¶”ì  ê²¬ê³ ì„±**: ì¡°ëª… ë³€í™”, ê°€ë ¤ì§ì— ê°•ê±´
4. **ì‹¤ì‹œê°„ ì²˜ë¦¬**: ìµœì í™” ì‹œ 30+ FPS ë‹¬ì„± ê°€ëŠ¥
5. **ì¶”ê°€ ë¹„ìš©**: í•˜ë“œì›¨ì–´ ì¶”ê°€ ì—†ì´ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œë§Œìœ¼ë¡œ êµ¬í˜„

**í•µì‹¬ ê¶Œì¥ì‚¬í•­**:
- Phase 1ë¶€í„° ë‹¨ê³„ì  êµ¬í˜„ (Depth â†’ Point Cloud â†’ Fusion â†’ ìµœì í™”)
- Kalman Filterë¡œ ë…¸ì´ì¦ˆ ì œê±° ë° ì˜ˆì¸¡ ëŠ¥ë ¥ í™•ë³´
- C++ + GPU ì‚¬ìš© ì‹œ ìµœê³  ì„±ëŠ¥ (60+ FPS)
- Adaptive fusionìœ¼ë¡œ ìƒí™©ë³„ ìµœì  ì„¼ì„œ í™œìš©

ì´ ì—°êµ¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬í˜„í•˜ë©´ í˜„ì¬ ì‹œìŠ¤í…œì˜ ì†ë„/ê°ë„ ì¸¡ì • ì •í™•ë„ë¥¼ íšê¸°ì ìœ¼ë¡œ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
